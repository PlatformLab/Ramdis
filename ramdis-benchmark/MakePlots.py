#!/usr/bin/env python

"""
Makes gnuplot graphs from files in a data directory. MakePlot.py figures out
based on the files in the directory which experiments have been run, generates
intermediate gnuplot formatted data files for those experiments, and then runs
gnuplot to generate graphs. When subsets of experiments are re-run, or more
experiments are run and data files are added to the directory, MakePlots.py is
smart enough to detect that and build only those graphs that don't yet exist or
are outdated.

Input Files:
   op_clientN-x_reqLatencies.dat
   op_clientN-x_execSummary.dat

Output Files:
   op_clientN-all_reqLatencies.cdf
   op_clientN-all_throughput.dat
   op_throughput_v_clients.dat
   op_throughput_v_clients.svg
"""

import sys
from os import listdir, makedirs
from os.path import isfile, join, getmtime, exists
from subprocess import call
from optparse import OptionParser
import re

def read_csv_into_list(filenameList):
    """
    Read csv files of floats, concatenate them all into a flat list and return
    them.
    """
    numbers = []
    for filename in filenameList:
        for line in open(filename, 'r'):
            if not re.match('([0-9]+\.[0-9]+) ', line):
                for value in line.split(","):
                    numbers.append(float(value))
    return numbers

def gen_cdf(filenameList, outputFilename):
    """
    Read data values from fileis given by filenameList, and produces a cdf in
    text form.  Each line in the printed output will contain a fraction and a
    number, such that the given fraction of all numbers in the log file have
    values less than or equal to the given number.
    """
    # Read the file into an array of numbers.
    numbers = read_csv_into_list(filenameList)

    # Output to the current file + .cdf
    outfile = open(outputFilename, 'w')

    # Generate a CDF from the array.
    numbers.sort()
    result = []
    outfile.write("%8.4f    %8.3f\n" % (0.0, 0.0))
    outfile.write("%8.4f    %8.3f\n" % (numbers[0], 1/len(numbers)))
    for i in range(1, 100):
        outfile.write("%8.4f    %8.3f\n" % (numbers[int(len(numbers)*i/100)], 
            i/100))
    outfile.write("%8.4f    %8.3f\n" % (numbers[int(len(numbers)*999/1000)], 
        .999))
    outfile.write("%8.4f    %9.4f\n" % (numbers[int(len(numbers)*9999/10000)], 
        .9999))
    outfile.write("%8.4f    %8.3f\n" % (numbers[-1], 1.0))
    outfile.close()

if __name__ == '__main__':
    parser = OptionParser(description=
            'Make GNUPlot files from data files generated by RamdisPerf',
            usage='%prog [options]',
            conflict_handler='resolve')

    parser.add_option('--dataDir', metavar='DIR', dest='data_dir',
            help='Directory of RamdisPerf data files.')

    (options, args) = parser.parse_args()

    if not options.data_dir:
        print "ERROR: Must specify location of data files with --dataDir"
        sys.exit()

    dataDir = options.data_dir
    graphDir = join(options.data_dir, 'graphs')

    # Create graph directory if it doesn't exist
    if not exists(graphDir):
        makedirs(graphDir)
    
    # First analyze directory to see which (op, clients) experiments are
    # present.
    experiments = {}
    allfiles = [f for f in listdir(dataDir) if isfile(join(dataDir, f))]
    for filename in allfiles:
        m = re.match('([a-z]+)_client([0-9]+)-1_reqLatencies.dat',
                filename)
        if m:
            if not m.group(1) in experiments:
                experiments[m.group(1)] = []
            experiments[m.group(1)].append(int(m.group(2)))

    print "Found data files for the following experiments:"
    for op in experiments.keys():
        experiments[op].sort()
        print "OP: %s, CLIENTS: %s" % (op, experiments[op])

    # Check to see what GNU plot files need to be generated or updated
    for op in experiments.keys():
        tputFilenameList = []
        for clients in experiments[op]:
            srcFilename = "%s_client%s-1_reqLatencies.dat" % (op, clients) 
            srcFileMTime = getmtime(join(dataDir, srcFilename))
            latCDFFilename = "%s_client%s-all_reqLatencies.cdf" % (op, clients)
            tputFilename = "%s_client%s-all_throughput.dat" % (op, clients)

            tputFilenameList.append(tputFilename)

            # Generate latency CDF for this experiment if needed.
            if latCDFFilename not in allfiles or srcFileMTime > getmtime(
                    join(dataDir, latCDFFilename)):
                if latCDFFilename not in allfiles:
                    print "Generating %s... " % latCDFFilename,
                else:
                    print "Updating %s... " % latCDFFilename,
                    
                sys.stdout.flush()

                reqLatFilenameList = []
                for i in range(1, clients + 1):
                    reqLatFilenameList.append(join(dataDir, 
                            "%s_client%d-%d_reqLatencies.dat" 
                            % (op, clients, i)))

                gen_cdf(reqLatFilenameList, join(dataDir, latCDFFilename))

                print "Done."

            # Generate throughput stat for this experiment if needed.
            if tputFilename not in allfiles or srcFileMTime > getmtime(
                    join(dataDir, tputFilename)):
                if tputFilename not in allfiles:
                    print "Generating %s... " % tputFilename,
                else:
                    print "Updating %s... " % tputFilename,
                sys.stdout.flush()

                execSumFilenameList = []
                for i in range(1, clients + 1):
                    execSumFilenameList.append(join(dataDir, 
                            "%s_client%d-%d_execSummary.dat" 
                            % (op, clients, i)))

                maxTotalTime = 0.0
                totalOps = 0
                for filename in execSumFilenameList:
                    sumDict = {}
                    for line in open(filename, 'r'):
                        cols = line.split(' ')
                        sumDict[cols[0]] = cols[1]
                    
                    if float(sumDict['totalTime']) > maxTotalTime:
                        maxTotalTime = float(sumDict['totalTime'])
                        
                    totalOps += int(sumDict['totalOps'])

                tputfile = open(join(dataDir, tputFilename), 'w')
                tputfile.write("%.2f\n" % (float(totalOps) / maxTotalTime))
                tputfile.close()

                print "Done."

        tputVClientsFilename = "%s_throughput_v_clients.dat" % (op)
        genTputVClients = False
        if tputVClientsFilename not in allfiles:
            genTputVClients = True
        else:
            for filename in tputFilenameList:
                if getmtime(join(dataDir, filename)) > getmtime(join(
                    dataDir, tputVClientsFilename)):
                    genTputVClients = True
                    break

        if genTputVClients == True:
            if tputVClientsFilename not in allfiles:
                print "Generating %s... " % tputVClientsFilename,
            else:
                print "Updating %s... " % tputVClientsFilename,
            sys.stdout.flush()

            tvcfile = open(join(dataDir, tputVClientsFilename), 'w')
            for clients in experiments[op]:
                tputFilename = "%s_client%s-all_throughput.dat" \
                        % (op, clients)
                tputfile = open(join(dataDir, tputFilename), 'r')
                tvcfile.write("%d %9.2f\n"
                        % (clients, float(tputfile.readline())))
                tputfile.close()

            tvcfile.close()

            print "Done."

    # Check to see what graphs need to be generated or updated
    graphfiles = [f for f in listdir(graphDir) if isfile(join(graphDir, f))]
    for op in experiments.keys():
        dependencyFilename = "%s_throughput_v_clients.dat" % (op)
        targetFilename = "%s_throughput_v_clients.svg" % (op)

        if targetFilename not in graphfiles or getmtime(join(dataDir,
            dependencyFilename)) > getmtime(join(graphDir, targetFilename)):
            if targetFilename not in graphfiles:
                print "Generating %s..." % targetFilename,
            else:
                print "Updating %s..." % targetFilename,
            sys.stdout.flush()

            gnuplotFilename = "%s_throughput_v_clients.gnu" % op
            call(["gnuplot", "-e", "inputFile='%s'; outputFile='%s'" %
                (join(dataDir, dependencyFilename), join(graphDir,
                    targetFilename)), gnuplotFilename])

            print "Done."
        
